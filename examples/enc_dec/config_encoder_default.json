{
    "architecture": "EncoderModel",
    "dtype": "bfloat16",
    "logits_dtype": "float32",
    "num_hidden_layers": 6,
    "num_attention_heads": 8,
    "hidden_size": 512,
    "norm_epsilon": 1e-06,
    "vocab_size": 32128,
    "position_embedding_type": "relative",
    "hidden_act": "relu",
    "quantization": {
        "quant_algo": null,
        "kv_cache_quant_algo": null
    },
    "mapping": {
        "world_size": 1,
        "tp_size": 1,
        "pp_size": 1
    },
    "use_parallel_embedding": false,
    "embedding_sharding_dim": 0,
    "share_embedding_table": false,
    "max_position_embeddings": 512,
    "num_key_value_heads": 8,
    "use_prompt_tuning": false,
    "head_size": 64,
    "has_position_embedding": false,
    "layernorm_type": 1,
    "has_attention_qkvo_bias": false,
    "has_mlp_bias": false,
    "has_model_final_layernorm": true,
    "has_embedding_layernorm": false,
    "has_embedding_scale": false,
    "ffn_hidden_size": 2048,
    "q_scaling": 0.125,
    "layernorm_position": 0,
    "mlp_type": 0,
    "relative_attention": true,
    "max_distance": 128,
    "num_buckets": 32,
    "model_type": "t5",
    "gated_act": false
}